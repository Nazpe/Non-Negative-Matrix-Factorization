{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization for Exploring the European Parliament's Topic Agenda\n",
    "### Assignment 2 for Machine Learning Complements class\n",
    "By Alexandra de Carvalho, Luís Costa, Nuno Pedrosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the needed Python libraries\n",
    "We will use Pandas for dataframe manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nunop\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\nunop\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nunop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nunop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nunop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nunop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# for modeling \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive sample.zip contains a sample corpus of 1,324 news articles divided into three time windows (month1, month2, month3).\n",
    "\n",
    "To run the code, please unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand pandas df column display width to enable easy inspection\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "# read the textfiles to a dataframe\n",
    "dir_path = 'sample' # folder path\n",
    "files = [] # list to store files\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        files.append(os.path.join(dir_path, path))\n",
    "    else:\n",
    "        subpath = os.path.join(dir_path, path)\n",
    "        for path2 in os.listdir(subpath):\n",
    "            if os.path.isfile(os.path.join(subpath, path2)):\n",
    "                files.append(os.path.join(subpath, path2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "To make all of the text in the speeches as comparable as possible we need to remove punctuation, capitalization, numbers, and strange characters. We also keep the term frequency on each document.\n",
    "\n",
    "With the before in mind, we will create text_tokens, that will be a dictionary of dictionarys. It is structured in such a way that, for each month and type sample key, we have a dictionary corresponding to the tokenized words with their count in the respective sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = dict()\n",
    "for filename in files:\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        text_tokens[filename] = dict()\n",
    "        \n",
    "        for line in lines:\n",
    "            for token in re.split('\\W+', str(line)):\n",
    "                token = token.lower()\n",
    "                if len(token) > 3 and not token.isnumeric() and not token.lower() in stopwords.words('english'):\n",
    "                    text_tokens[filename][token] = text_tokens[filename].get(token, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important step is to lemmatize the words, that is to convert verbs, adjetives, and others variations of the same word into their base form. This allows us to analyse related words as a single one and reduces the number of words in the documents matrix, reducing sparcity.\n",
    "\n",
    "Ex: walked -> walk\n",
    "\n",
    "With the before in mind, we will create nouns, that will be a dictionary of dictionarys. It is structured in a similar way as text_tokens, but with the count by lemmatized word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()   # stored function to lemmatize each word\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "nouns = dict()\n",
    "for filename, tokens in text_tokens.items():\n",
    "    if filename not in nouns:\n",
    "        nouns[filename] = dict()\n",
    "\n",
    "    for (word, pos) in pos_tag(list(tokens.keys())):\n",
    "        if is_noun(pos):\n",
    "            nouns[filename][wordnet_lemmatizer.lemmatize(word)] = nouns[filename].get(wordnet_lemmatizer.lemmatize(word), 0) + text_tokens[filename][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A is a matrix where every line corresponds to a specific document, and every column corresponds to a specific term/token, in this way, we get a matrix with the menbership of each term in each document. In a initial state, these menberships will be represented with it's term frequency weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "a = dictvectorizer.fit_transform(list(nouns.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1324, 11236)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1324 documents and 11236 different terms\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the list of all tokens (all columns of A, in order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = dictvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF: (term frequency–inverse document frequency) term weighting  \n",
    "\n",
    "The tf–idf value of a word increases proportionally as the number of occurrences of it in a document increases, however, this value is balanced by the normal frequency of that word. This helps to distinguish the fact that the occurrence of some words is generally more common than others.\n",
    "\n",
    "This also helps to produce diverse but semantically coherent topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating the TF-IDF weights and updating the A matrix with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_idx in range(len(token_list)):\n",
    "    idf = math.log(len(a[:, column_idx])/len([x for x in a[:, column_idx] if x != 0]), 10)\n",
    "\n",
    "    for element_idx in range(len(files)):\n",
    "        if a[element_idx,column_idx] != 0:\n",
    "            a[element_idx,column_idx] = (math.log(a[element_idx,column_idx], 10) + 1) * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best value for K : TC-W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same number of terms per topic *t* as the paper (10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of terms per topic\n",
    "t = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best number of topics k, we will use a topic coherence measure called TC-W2V. In this approach, the coherence of each topic is measured using the cosine similarity. The coherence of a model is the mean coherence of the topics of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the NMF with a number of topics that ranges between 10 and 25 in order to find the ammount of topics that gives us a model with the biggest coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  10 . Model coherence: 0.0023823067576934894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  11 . Model coherence: 0.005569110116498036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  12 . Model coherence: 0.0019128068626202918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  13 . Model coherence: 0.0010181541849150618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  14 . Model coherence: 0.0020709820335642207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  15 . Model coherence: 0.004355091831336419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  16 . Model coherence: 0.0024432344284529488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  17 . Model coherence: 0.0030270003293659175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  18 . Model coherence: 1.6408381742183745e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  19 . Model coherence: 0.0033938303584374525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  20 . Model coherence: 0.0030031270694194567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  21 . Model coherence: 0.002810686057502472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  22 . Model coherence: -0.00017979214617022945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  23 . Model coherence: 0.00273876978525361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  24 . Model coherence: 0.001925166782112447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  25 . Model coherence: 0.0035117933278282485\n"
     ]
    }
   ],
   "source": [
    "max_model_coherence = 0\n",
    "res_k = 0\n",
    "\n",
    "for k in range(10,26):\n",
    "\n",
    "    nmf_model = NMF(k, random_state=1) \n",
    "    nmf_model.fit_transform(a)\n",
    "\n",
    "    vocabulary = [[token_list[x[1]] for x in sorted(zip(topic,range(len(topic))), reverse = True)[:t]] for topic in nmf_model.components_]\n",
    "    model = Word2Vec(sentences = vocabulary, vector_size = 200, window = 5, hs = 1, negative = 0, min_count = 1)\n",
    "    \n",
    "    # calculating individual topic coherence scores for each topic\n",
    "    model_score = []\n",
    "    for topic in vocabulary:\n",
    "        topic_score = []\n",
    "        for w1 in topic:\n",
    "            for w2 in topic:\n",
    "                if w2 > w1:\n",
    "                    word_score = cosine_similarity(model.wv[w2].reshape(1,-1),model.wv[w1].reshape(1,-1))[0]\n",
    "                    topic_score.append(word_score[0])\n",
    "        \n",
    "        topic_score = sum(topic_score)/len(topic_score) # mean of each word pair similarity in the topic\n",
    "        model_score.append(topic_score)\n",
    "\n",
    "    model_coherence = sum(model_score)/len(model_score) # mean of topic coherence in the model\n",
    "    print(\"k = \",k, \". Model coherence:\", model_coherence)\n",
    "\n",
    "    # used in order to choose the number of topics that has the biggest coherence\n",
    "    if model_coherence > max_model_coherence:\n",
    "        max_model_coherence = model_coherence\n",
    "        res_k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res_k is the best number of topics obtained\n",
    "res_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain topics that suit our data, we will use NMF (Non-negative matrix factorization).\n",
    "\n",
    "It is a group of algorithms where a matrix A is factored into two matrices, one W and one H, with the property that the matrices have no negative elements. This makes the resulting matrices easier to analyze.\n",
    "\n",
    "General formula:\n",
    "V = W * H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The W and H matrices have special features:\n",
    "\n",
    "\n",
    "- The k lines of H represent the k topics defined by non-negative weights, the columns represent the terms. This gives the rank of the term in the topics: the higher the value, the higher the rank of the word in this topic. \n",
    "Sorting a line gives us the description of the topic: a ranking of the most related terms, allowing interpretation of the themes.\n",
    "\n",
    "- The k columns of W the topics, and the lines represents the documents/speeches. The values represent the weights of the menbership of each document in a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of topics that gives us the biggest coherence is 11, so that is the number of topics that we will use in our definitive NMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nunop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(res_k, random_state=1) \n",
    "w = nmf_model.fit_transform(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will analyse the constitution of the obtained topics:\n",
    "\n",
    "For each topic, find the t higher weights index and find the correpondent token (same index) in the token list. These are the descriptors of each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['technology', 'phone', 'video', 'speed', 'generation', 'device', 'network', 'broadband', 'image', 'picture']\n",
      "Topic 1 : ['club', 'player', 'football', 'team', 'chelsea', 'game', 'season', 'manager', 'champion', 'league']\n",
      "Topic 2 : ['election', 'blair', 'party', 'minister', 'government', 'leader', 'tory', 'secretary', 'chancellor', 'democrat']\n",
      "Topic 3 : ['music', 'band', 'song', 'rock', 'artist', 'album', 'singer', 'record', 'single', 'award']\n",
      "Topic 4 : ['forsyth', 'frederick', 'terrorist', 'internment', 'forsythe', 'totalitarianism', 'qaeda', 'fundamentalism', 'churchill', 'liberty']\n",
      "Topic 5 : ['growth', 'economy', 'market', 'price', 'rate', 'rise', 'bank', 'investment', 'analyst', 'dollar']\n",
      "Topic 6 : ['angel', 'rhapsody', 'bland', 'brit', 'guy', 'pulp', 'cheesy', 'deserve', 'joss', 'joke']\n",
      "Topic 7 : ['sub', 'minute', 'goal', 'ball', 'yard', 'header', 'kick', 'cech', 'duff', 'cross']\n",
      "Topic 8 : ['software', 'virus', 'user', 'mail', 'program', 'computer', 'security', 'site', 'information', 'attack']\n",
      "Topic 9 : ['court', 'yukos', 'bankruptcy', 'gazprom', 'case', 'fraud', 'russia', 'rosneft', 'khodorkovsky', 'unit']\n",
      "Topic 10 : ['film', 'actor', 'award', 'oscar', 'star', 'actress', 'comedy', 'movie', 'nomination', 'ceremony']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(\"Topic\", i, \":\",[token_list[x[1]] for x in sorted(zip(topic,range(len(topic))), reverse = True)[:t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the obtained topics, we can see that they correspond to a specific theme, showing that they were well obtained.\n",
    "For example, in topic 3, we have just terms related to music, topic 5 has terms related to economy and topic 10 has terms related to movies. (These results can change if we run the algorithm again.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can then observe the documents with bigger weights for each topic. Because the files names already tag the contained speech by topic, we can infer the validity of the model built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['sample\\\\month1\\\\tech_032.txt', 'sample\\\\month3\\\\tech_335.txt', 'sample\\\\month3\\\\tech_294.txt', 'sample\\\\month2\\\\tech_155.txt', 'sample\\\\month3\\\\tech_396.txt', 'sample\\\\month1\\\\tech_009.txt', 'sample\\\\month3\\\\tech_309.txt', 'sample\\\\month3\\\\tech_313.txt', 'sample\\\\month1\\\\tech_094.txt', 'sample\\\\month3\\\\tech_216.txt']\n",
      "Topic 1 : ['sample\\\\month1\\\\football_018.txt', 'sample\\\\month1\\\\football_027.txt', 'sample\\\\month3\\\\football_207.txt', 'sample\\\\month1\\\\football_024.txt', 'sample\\\\month1\\\\football_088.txt', 'sample\\\\month1\\\\football_087.txt', 'sample\\\\month3\\\\football_202.txt', 'sample\\\\month3\\\\football_223.txt', 'sample\\\\month2\\\\football_180.txt', 'sample\\\\month1\\\\football_086.txt']\n",
      "Topic 2 : ['sample\\\\month3\\\\politics_253.txt', 'sample\\\\month3\\\\politics_218.txt', 'sample\\\\month3\\\\politics_255.txt', 'sample\\\\month3\\\\politics_252.txt', 'sample\\\\month3\\\\politics_256.txt', 'sample\\\\month3\\\\politics_254.txt', 'sample\\\\month3\\\\politics_260.txt', 'sample\\\\month3\\\\politics_251.txt', 'sample\\\\month3\\\\politics_257.txt', 'sample\\\\month1\\\\politics_041.txt']\n",
      "Topic 3 : ['sample\\\\month3\\\\entertainment_256.txt', 'sample\\\\month2\\\\entertainment_131.txt', 'sample\\\\month3\\\\entertainment_229.txt', 'sample\\\\month2\\\\entertainment_142.txt', 'sample\\\\month3\\\\entertainment_263.txt', 'sample\\\\month3\\\\entertainment_262.txt', 'sample\\\\month3\\\\entertainment_244.txt', 'sample\\\\month3\\\\entertainment_264.txt', 'sample\\\\month3\\\\entertainment_236.txt', 'sample\\\\month3\\\\entertainment_255.txt']\n",
      "Topic 4 : ['sample\\\\month3\\\\politics_290.txt', 'sample\\\\month2\\\\politics_160.txt', 'sample\\\\month3\\\\politics_224.txt', 'sample\\\\month1\\\\politics_052.txt', 'sample\\\\month1\\\\politics_040.txt', 'sample\\\\month3\\\\politics_221.txt', 'sample\\\\month3\\\\politics_205.txt', 'sample\\\\month3\\\\politics_214.txt', 'sample\\\\month3\\\\politics_263.txt', 'sample\\\\month2\\\\politics_151.txt']\n",
      "Topic 5 : ['sample\\\\month3\\\\business277.txt', 'sample\\\\month3\\\\business245.txt', 'sample\\\\month3\\\\business287.txt', 'sample\\\\month3\\\\business290.txt', 'sample\\\\month3\\\\business262.txt', 'sample\\\\month2\\\\business163.txt', 'sample\\\\month3\\\\business249.txt', 'sample\\\\month1\\\\business044.txt', 'sample\\\\month2\\\\business119.txt', 'sample\\\\month2\\\\business159.txt']\n",
      "Topic 6 : ['sample\\\\month3\\\\entertainment_253.txt', 'sample\\\\month3\\\\entertainment_256.txt', 'sample\\\\month3\\\\entertainment_251.txt', 'sample\\\\month3\\\\entertainment_201.txt', 'sample\\\\month1\\\\entertainment_013.txt', 'sample\\\\month2\\\\football_198.txt', 'sample\\\\month2\\\\entertainment_128.txt', 'sample\\\\month3\\\\entertainment_215.txt', 'sample\\\\month2\\\\entertainment_106.txt', 'sample\\\\month2\\\\entertainment_191.txt']\n",
      "Topic 7 : ['sample\\\\month1\\\\football_010.txt', 'sample\\\\month3\\\\football_253.txt', 'sample\\\\month2\\\\football_152.txt', 'sample\\\\month3\\\\football_246.txt', 'sample\\\\month1\\\\football_045.txt', 'sample\\\\month2\\\\football_174.txt', 'sample\\\\month2\\\\football_184.txt', 'sample\\\\month1\\\\football_069.txt', 'sample\\\\month1\\\\football_011.txt', 'sample\\\\month3\\\\football_260.txt']\n",
      "Topic 8 : ['sample\\\\month3\\\\tech_299.txt', 'sample\\\\month3\\\\tech_270.txt', 'sample\\\\month1\\\\tech_077.txt', 'sample\\\\month3\\\\tech_281.txt', 'sample\\\\month3\\\\tech_308.txt', 'sample\\\\month1\\\\tech_083.txt', 'sample\\\\month3\\\\tech_357.txt', 'sample\\\\month3\\\\tech_398.txt', 'sample\\\\month3\\\\tech_227.txt', 'sample\\\\month1\\\\tech_034.txt']\n",
      "Topic 9 : ['sample\\\\month2\\\\business192.txt', 'sample\\\\month1\\\\business083.txt', 'sample\\\\month1\\\\business025.txt', 'sample\\\\month3\\\\business299.txt', 'sample\\\\month2\\\\business164.txt', 'sample\\\\month2\\\\business127.txt', 'sample\\\\month3\\\\business230.txt', 'sample\\\\month1\\\\business003.txt', 'sample\\\\month1\\\\business077.txt', 'sample\\\\month2\\\\business181.txt']\n",
      "Topic 10 : ['sample\\\\month1\\\\entertainment_095.txt', 'sample\\\\month1\\\\entertainment_064.txt', 'sample\\\\month1\\\\entertainment_038.txt', 'sample\\\\month3\\\\entertainment_275.txt', 'sample\\\\month1\\\\entertainment_082.txt', 'sample\\\\month1\\\\entertainment_039.txt', 'sample\\\\month1\\\\entertainment_069.txt', 'sample\\\\month1\\\\entertainment_051.txt', 'sample\\\\month1\\\\entertainment_078.txt', 'sample\\\\month1\\\\entertainment_086.txt']\n"
     ]
    }
   ],
   "source": [
    "for i in range(res_k):\n",
    "    print(\"Topic\", i, \":\",[files[x[1]].split('/')[-1] for x in sorted(zip(w[:,i],range(len(w[:,i]))), reverse = True)[:t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, theres is definitively a match between the terms in each topic and the name of the speechs from which they were recovered. This results are aligned with what was expected and as such verify that the model built is a valid one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the objective of comparing our results, we will apply LDA to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the speeches\n",
    "text_tokens = []\n",
    "for filename in files:\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        sup_list = []\n",
    "        for line in lines:\n",
    "            for token in re.split('\\W+', str(line)):\n",
    "                token = token.lower()\n",
    "                if len(token) > 3 and not token.isnumeric() and not token.lower() in stopwords.words('english'):\n",
    "                    sup_list.append(token)\n",
    "    text_tokens.append(sup_list)\n",
    "\n",
    "for doc in text_tokens:\n",
    "    doc = [wordnet_lemmatizer.lemmatize(x) for x in doc]\n",
    "\n",
    "# only used if the file is run on macOs due to the .DS_Store file\n",
    "# text_tokens.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.014*\"said\" + 0.012*\"people\" + 0.006*\"could\" + 0.005*\"games\" + 0.005*\"technology\" + 0.005*\"users\" + 0.004*\"microsoft\" + 0.004*\"also\" + 0.004*\"online\" + 0.004*\"software\" \n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.022*\"said\" + 0.013*\"would\" + 0.011*\"government\" + 0.009*\"labour\" + 0.008*\"election\" + 0.008*\"blair\" + 0.008*\"party\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"howard\" \n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.011*\"said\" + 0.007*\"game\" + 0.007*\"would\" + 0.007*\"club\" + 0.006*\"chelsea\" + 0.006*\"united\" + 0.006*\"players\" + 0.005*\"arsenal\" + 0.005*\"league\" + 0.005*\"time\" \n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.013*\"best\" + 0.011*\"film\" + 0.011*\"music\" + 0.008*\"said\" + 0.008*\"show\" + 0.007*\"year\" + 0.006*\"also\" + 0.006*\"band\" + 0.006*\"awards\" + 0.005*\"award\" \n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.018*\"said\" + 0.009*\"mail\" + 0.007*\"people\" + 0.007*\"spam\" + 0.006*\"virus\" + 0.005*\"site\" + 0.005*\"also\" + 0.005*\"many\" + 0.004*\"sites\" + 0.004*\"attacks\" \n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.028*\"said\" + 0.007*\"would\" + 0.005*\"people\" + 0.004*\"police\" + 0.004*\"rights\" + 0.004*\"told\" + 0.004*\"government\" + 0.003*\"could\" + 0.003*\"also\" + 0.003*\"home\" \n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.014*\"mobile\" + 0.009*\"said\" + 0.008*\"phone\" + 0.007*\"software\" + 0.006*\"phones\" + 0.004*\"would\" + 0.004*\"time\" + 0.004*\"also\" + 0.004*\"could\" + 0.004*\"computer\" \n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.015*\"said\" + 0.006*\"lord\" + 0.006*\"would\" + 0.006*\"people\" + 0.005*\"could\" + 0.004*\"ebbers\" + 0.004*\"bill\" + 0.004*\"told\" + 0.003*\"worldcom\" + 0.003*\"lords\" \n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.006*\"first\" + 0.005*\"year\" + 0.005*\"chart\" + 0.004*\"budget\" + 0.003*\"said\" + 0.003*\"world\" + 0.003*\"also\" + 0.003*\"last\" + 0.003*\"sales\" + 0.003*\"part\" \n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.015*\"said\" + 0.010*\"brown\" + 0.009*\"blair\" + 0.006*\"minister\" + 0.006*\"prime\" + 0.006*\"labour\" + 0.005*\"would\" + 0.004*\"book\" + 0.004*\"told\" + 0.004*\"chancellor\" \n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.018*\"said\" + 0.011*\"year\" + 0.006*\"market\" + 0.005*\"also\" + 0.005*\"growth\" + 0.005*\"last\" + 0.005*\"economy\" + 0.005*\"company\" + 0.005*\"bank\" + 0.005*\"would\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn the tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(text_tokens)\n",
    "\n",
    "# Convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in text_tokens]\n",
    "\n",
    "# We will use the same number of topics as in the NMF so we can compare\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=11, id2word=dictionary, passes=20)\n",
    "\n",
    "# Printing the results\n",
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, the LDA model returns terms such as \"would\", \"said\" and \"also\" which are speech conectors and verbs that not really related to the topics produced. . As such we can conclude that the NMF approach produces more fine grained results, being able to move past the noise present in the text and to really capture the most common terms per topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1cd21b2f3a99e4553639ab98bc29e605a9bde5dbe77b44941113c250087c944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
